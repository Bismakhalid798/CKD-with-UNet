{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Bisma Khalid\n\n19I-1797\n\nAI - K\n","metadata":{"id":"sW_pQYXxVnRS"}},{"cell_type":"markdown","source":"Assignment 3","metadata":{"id":"El-VXPZRVtxc"}},{"cell_type":"code","source":"import os\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Recall, Precision,Accuracy\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport cv2\nfrom glob import glob\nfrom sklearn.utils import shuffle\nimport tensorflow as tf","metadata":{"id":"EOv6Wr2PwfEV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H=256\nW=256\ndim=(H,W)","metadata":{"id":"X38RAMHRYcaU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nDefining the Structure of the Model","metadata":{"id":"5SHu5-AeVzwa"}},{"cell_type":"code","source":"#This convolution part will also be included in encoder and decoder part of the network\ndef Conv_Block(input,num_of_filter):\n    x=Conv2D(num_of_filter,3,padding=\"same\")(input)\n    x=BatchNormalization()(x)\n    x=Activation('ReLU')(x)\n\n    x=Conv2D(num_of_filter,3,padding=\"same\")(input)\n    x=BatchNormalization()(x)\n    x=Activation('ReLU')(x)\n  \n    return x\n\n\ndef Encoder(input,num_of_filter):\n    x = Conv_Block(input, num_of_filter) #Skip connection\n    p = MaxPool2D((2, 2))(x)  #Feature selection\n    return x, p\n\ndef Decoder(input, skip_features, num_of_filter):\n    x = Conv2DTranspose(num_of_filter, (2, 2), strides=2, padding=\"same\")(input)\n    x = Concatenate()([x, skip_features])\n    x = Conv_Block(x, num_of_filter)\n    return x","metadata":{"id":"0Mz0TB58ugvV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Customized Encoder - Decoder Model","metadata":{"id":"QPvfIaX5WAHU"}},{"cell_type":"code","source":"def Model_Build(input_shape):\n    inputs=Input(input_shape)\n    #as you go down in the encoder,resolution decreases , number of filters doubles\n    s1,p1=Encoder(inputs,16)\n    s2,p2=Encoder(p1,32)\n    s3,p3=Encoder(p2,64)\n    s4,p4=Encoder(p3,128)\n    s5,p5=Encoder(p4,256)\n    #s6,p6=Encoder(p5,128)\n\n    b1=Conv_Block(p5,512)\n\n    d1 = Decoder(b1, s5, 256) \n    d2 = Decoder(d1, s4, 128) \n    d3 = Decoder(d2, s3, 64)\n    d4 = Decoder(d3, s2, 32)\n    d5 = Decoder(d4, s1, 15)\n#     d6 = Decoder(d5, s1, 4)\n\n    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d5)  #our data is in grayscale, gives 0,1 output\n\n    model = Model(inputs, outputs, name=\"Encoder-Decoder\")\n    return model\n","metadata":{"id":"RO_TwOgvwXDt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (256, 256, 3)\nmodel = Model_Build(input_shape)\nmodel.summary()","metadata":{"id":"CaYmM2UiwxHI","outputId":"42a849da-890f-4015-b311-45fa4ec45cf5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\ntf.random.set_seed(42)\nbatch_size = 32\nlr = 0.01\nnum_epochs = 15","metadata":{"id":"Up0Z1ovywxJH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Metrices","metadata":{"id":"e1do0DKjWGzj"}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef iou(y_true, y_pred):\n    def f(y_true, y_pred):\n        intersection = (y_true * y_pred).sum()\n        union = y_true.sum() + y_pred.sum() - intersection\n        x = (intersection + 1e-15) / (union + 1e-15)\n        x = x.astype(np.float32)\n        return x\n    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n\nsmooth = 1e-15\ndef dice_coef(y_true, y_pred):\n    y_true = tf.keras.layers.Flatten()(y_true)\n    y_pred = tf.keras.layers.Flatten()(y_pred)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred)","metadata":{"id":"oqwQdF6GxxkA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_weights = os.path.join(\"/kaggle/working/\", \"model_dense.h5\")\ncsv_weights = os.path.join(\"/kaggle/working/\", \"bisma_dense.csv\")\n \n\ncallbacks = [\n    ModelCheckpoint(model_weights, verbose=1, save_best_only=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.00001, patience=2, min_lr=1e-7, verbose=1),\n    CSVLogger(csv_weights),\n    TensorBoard(),\n    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=False),\n    ]","metadata":{"id":"wL0gVqBvwxPd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Loading","metadata":{"id":"dLrXYwElWPNn"}},{"cell_type":"code","source":"# path=\"/content/drive/MyDrive/FYP/Dataset/PNG_Slices_Segmented\"\npath=\"/kaggle/input/kidneykits19/PNG_Slices_Segmented/PNG_Slices_Segmented\"","metadata":{"id":"_mUC5cE7z2Fs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(file_path):\n    #Loading the image files\n    images = sorted(glob(f\"{file_path}/*/Images/*.png\"))\n    segmentations=sorted(glob(f\"{file_path}/*/Segmentation/*.png\"))\n    print(len(images),len(segmentations))\n\n\n  # #Spliting the data into training and testing using builtin libraries\n    split=0.2\n    split_size = int(len(images) * split)\n    train_x, valid_x = train_test_split(images, test_size=split, random_state=42)\n    train_y, valid_y = train_test_split(segmentations, test_size=split, random_state=42)\n\n    return (train_x, train_y), (valid_x, valid_y)\n \n\n\ndef read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x=cv2.resize(x, dim)\n    x = x/255.0\n    x = x.astype(np.float32)\n    return x\n\ndef read_mask(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x=cv2.resize(x, dim)\n    x = x/255.0\n    x = x > 0.5\n    x = x.astype(np.float32)\n    x = np.expand_dims(x, axis=-1)\n    return x\n\ndef shuffling(x, y):\n    x, y = shuffle(x, y, random_state=42)\n    return x, y\n\ndef tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x)\n        y = read_mask(y)\n        return x, y\n\n    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n    x.set_shape([H, W, 3])\n    y.set_shape([H, W, 1])\n    return x, y\n\ndef tf_dataset(x, y, batch=10):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.map(tf_parse)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(10)\n    return dataset","metadata":{"id":"xCO_egG3z2Jy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train_x, train_y), (valid_x, valid_y) = load_data(path)\nprint(len(train_x),len(train_y))\nprint(len(valid_x),len(valid_y))\ntrain_x, train_y = shuffling(train_x, train_y)","metadata":{"id":"JgI-krHgz2Mj","outputId":"e7c62772-42c2-4b42-de10-e3ddbbae4239","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\ntrain_dataset = tf_dataset(train_x, train_y, batch=batch_size)\nvalid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)","metadata":{"id":"GsZTZseY01Tm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Training","metadata":{"id":"AP6Fx2pNWUzV"}},{"cell_type":"code","source":"# model = Model_Build((512, 512, 3))\n# metrics = [dice_coef, iou, Recall(), Precision(),Accuracy()]\nmse = tf.keras.losses.MeanSquaredError()\nmetrics = [dice_coef, iou,Accuracy()]\nmodel.compile(loss=dice_loss, optimizer=Adam(lr), metrics=metrics)","metadata":{"id":"ZyC1PFTTwxNA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.fit(train_dataset,epochs=num_epochs,validation_data=valid_dataset,shuffle=False)\nmodel.fit(train_dataset,epochs=num_epochs,validation_data=valid_dataset,callbacks=callbacks,shuffle=False )","metadata":{"id":"uqjlNCaLyXWi","outputId":"f0db5719-80d5-422b-c7f6-ce0e04a9f991","trusted":true},"execution_count":null,"outputs":[]}]}